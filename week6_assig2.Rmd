---
title: "Week 6 - Market Segmentation for Airlines"
author: "Vishal Gupta"
date: "3/31/2019"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=3,fig.align = "center")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
```

#### Problem 1.1 - Normalizing the Data

Read the dataset AirlinesCluster.csv into R and call it "airlines".

Looking at the summary of airlines, which TWO variables have (on average) the smallest values?
```{r}
airlines = read.csv("~/Downloads/AirlinesCluster.csv")
summary(airlines)
```


- Balance
- QualMiles
- BonusMiles
- **BonusTrans**
- FlightMiles
- **FlightTrans**
- DaysSinceEnroll

Which TWO variables have (on average) the largest values?


- **Balance**
- QualMiles
- **BonusMiles**
- BonusTrans
- FlightMiles
- FlightTrans
- DaysSinceEnroll

#### Problem 1.2 - Normalizing the Data

In this problem, we will normalize our data before we run the clustering algorithms. Why is it important to normalize the data before clustering?


- If we don't normalize the data, the clustering algorithms will not work (we will get an error in R).
- If we don't normalize the data, it will be hard to interpret the results of the clustering.
- **If we don't normalize the data, the clustering will be dominated by the variables that are on a larger scale.**
- If we don't normalize the data, the clustering will be dominated by the variables that are on a smaller scale.



#### Problem 1.3 - Normalizing the Data

Let's go ahead and normalize our data. You can normalize the variables in a data frame by using the preProcess function in the "caret" package. You should already have this package installed from Week 4, but if not, go ahead and install it with install.packages("caret"). Then load the package with library(caret).

Now, create a normalized data frame called "airlinesNorm" by running the following commands:
```{r}
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)
summary(airlinesNorm)
```
The first command pre-processes the data, and the second command performs the normalization. If you look at the summary of airlinesNorm, you should see that all of the variables now have mean zero. You can also see that each of the variables has standard deviation 1 by using the sd() function.

In the normalized data, which variable has the largest maximum value?

- Balance
- QualMiles
- BonusMiles
- BonusTrans
- **FlightMiles**
- FlightTrans
- DaysSinceEnroll

In the normalized data, which variable has the smallest minimum value?


- Balance
- QualMiles
- BonusMiles
- BonusTrans
- FlightMiles
- FlightTrans
- **DaysSinceEnroll**




#### Problem 2.1 - Hierarchical Clustering

Compute the distances between data points (using euclidean distance) and then run the Hierarchical clustering algorithm (using method="ward.D") on the normalized data. It may take a few minutes for the commands to finish since the dataset has a large number of observations for hierarchical clustering.
```{r}
distances = dist(airlinesNorm, method = "euclidean")
clusterAirlines = hclust(distances, method = "ward.D") 
```

Then, plot the dendrogram of the hierarchical clustering process. Suppose the airline is looking for somewhere between 2 and 10 clusters. According to the dendrogram, which of the following is NOT a good choice for the number of clusters?
```{r}
plot(clusterAirlines)
```


- 2
- 3
- **6**
- 7


#### Problem 2.2 - Hierarchical Clustering

Suppose that after looking at the dendrogram and discussing with the marketing department, the airline decides to proceed with 5 clusters. Divide the data points into 5 clusters by using the cutree function. How many data points are in Cluster 1?


```{r}
clusterGroups = cutree(clusterAirlines, k = 5)
table(clusterGroups)
```

#### Problem 2.3 - Hierarchical Clustering

Now, use tapply to compare the average values in each of the variables for the 5 clusters (the centroids of the clusters). You may want to compute the average values of the unnormalized data so that it is easier to interpret. You can do this for the variable "Balance" with the following command:
```
tapply(airlines$Balance, clusterGroups, mean)
```
Compared to the other clusters, Cluster 1 has the largest average values in which variables (if any)? Select all that apply.

```{r}
airlines$HeirClusters = clusterGroups
airlines %>% group_by(HeirClusters) %>% summarise_all(list(avg = mean))
```

- **Balance**
- QualMiles
- BonusMiles
- BonusTrans
- FlightMiles
- FlightTrans
- **DaysSinceEnroll**
- None

How would you describe the customers in Cluster 1?


- Relatively new customers who don't use the airline very often.
- **Infrequent but loyal customers.**
- Customers who have accumulated a large amount of miles, mostly through non-flight transactions.
- Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.
- Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.


#### Problem 2.4 - Hierarchical Clustering

Compared to the other clusters, Cluster 2 has the largest average values in which variables (if any)? Select all that apply.


- Balance
- **QualMiles**
- BonusMiles
- BonusTrans
- **FlightMiles**
- **FlightTrans**
- DaysSinceEnroll
- None

How would you describe the customers in Cluster 2?


- Relatively new customers who don't use the airline very often.
- Infrequent but loyal customers.
- Customers who have accumulated a large amount of miles, mostly through non-flight transactions.
- **Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.**
- Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.




#### Problem 2.5 - Hierarchical Clustering

Compared to the other clusters, Cluster 3 has the largest average values in which variables (if any)? Select all that apply.


- Balance
- QualMiles
- **BonusMiles**
- **BonusTrans**
- FlightMiles
- FlightTrans
- DaysSinceEnroll
- None

How would you describe the customers in Cluster 3?


- Relatively new customers who don't use the airline very often.
- Infrequent but loyal customers.
- **Customers who have accumulated a large amount of miles, mostly through non-flight transactions.**
- Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.
- Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.

#### Problem 2.6 - Hierarchical Clustering

Compared to the other clusters, Cluster 4 has the largest average values in which variables (if any)? Select all that apply.


- Balance
- QualMiles
- BonusMiles
- BonusTrans
- FlightMiles
- FlightTrans
- DaysSinceEnroll
- **None**

How would you describe the customers in Cluster 4?


- Relatively new customers who don't use the airline very often.
- Infrequent but loyal customers.
- Customers who have accumulated a large amount of miles, mostly through non-flight transactions.
- Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.
- **Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.**


#### Problem 2.7 - Hierarchical Clustering

Compared to the other clusters, Cluster 5 has the largest average values in which variables (if any)? Select all that apply.


- Balance
- QualMiles
- BonusMiles
- BonusTrans
- FlightMiles
- FlightTrans
- DaysSinceEnroll
- **None**

How would you describe the customers in Cluster 5?


- Relatively new customers who don't use the airline very often.
- Infrequent but loyal customers.
- Customers who have accumulated a large amount of miles, mostly through non-flight transactions.
- Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.
- **Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.**


#### Problem 3.1 - K-Means Clustering

Now run the k-means clustering algorithm on the normalized data, again creating 5 clusters. Set the seed to 88 right before running the clustering algorithm, and set the argument iter.max to 1000.

```{r}
set.seed(88)
clustersKmeans = kmeans(airlinesNorm,5,iter.max = 1000)
clusterAirlineKMGroups = as.factor(clustersKmeans$cluster)
```

How many clusters have more than 1,000 observations?

```{r}
table(clusterAirlineKMGroups)
```

#### Problem 3.2 - K-Means Clustering

Now, compare the cluster centroids to each other either by dividing the data points into groups and then using tapply, or by looking at the output of kmeansClust$centers, where "kmeansClust" is the name of the output of the kmeans function. (Note that the output of kmeansClust$centers will be for the normalized data. If you want to look at the average values for the unnormalized data, you need to use tapply like we did for hierarchical clustering.)

Do you expect Cluster 1 of the K-Means clustering output to necessarily be similar to Cluster 1 of the Hierarchical clustering output?


- Yes, because the clusters are displayed in order of size, so the largest cluster will always be first.
- Yes, because the clusters are displayed according to the properties of the centroid, so the cluster order will be similar.
- **No, because cluster ordering is not meaningful in either k-means clustering or hierarchical clustering.**
- No, because the clusters produced by the k-means algorithm will never be similar to the clusters produced by the Hierarchical algorithm.